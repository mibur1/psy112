{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelling Exercises\n",
    "\n",
    "## Exercise 1: Model Selection\n",
    "\n",
    "Today we are working with the `California Housing dataset`, which you are already familiar with, as we previously used it while exploring resampling method.\n",
    "This dataset is based on the 1990 U.S. Census and includes features describing California districts. \n",
    "\n",
    "1) Familiarize yourself with the data\n",
    "    - What kind of features are in the dataset? What is the target?\n",
    "2) Baseline model \n",
    "    - Create a baseline linear regression model using **all** features and evaluate the model through 5-fold cross validation, using R² as the performance metric\n",
    "    - Print the individual and average R²\n",
    "3) Apply a forward stepwise selection to find a simpler suitable model.\n",
    "    - Split the data into 80% training data and 20% testing data (print the shape to confirm it was sucessful)\n",
    "    - Perform a forward stepwise selection with a linear regression model, 5-fold CV, R² score, and `parsimonious` feature selection (refer to [documentation](https://rasbt.github.io/mlxtend/api_subpackages/mlxtend.feature_selection/) for further information)\n",
    "    - Print the best CV R² as well as the chosen features\n",
    "4) Evaluate the model on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R² scores from each fold: [0.54866323 0.46820691 0.55078434 0.53698703 0.66051406]\n",
      "Average R² score: 0.5530311140279571\n",
      "(16512, 8) (4128, 8)\n",
      "(16512,) (4128,)\n",
      ">> Forward SFS:\n",
      "   Best CV R²      : 0.612\n",
      "   Optimal # feats : 7\n",
      "   Feature names   : ('MedInc', 'HouseAge', 'AveRooms', 'AveBedrms', 'AveOccup', 'Latitude', 'Longitude')\n",
      "Test R² for the sfs model: 0.5757\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from mlxtend.feature_selection import SequentialFeatureSelector\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "# 1) Load the California housing dataset\n",
    "data = fetch_california_housing(as_frame=True)\n",
    "\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "\n",
    "# 2) Create baseline model \n",
    "model = LinearRegression()\n",
    "scores = cross_val_score(model, X, y, cv=5, scoring='r2')\n",
    "\n",
    "# Print the results\n",
    "print(\"R² scores from each fold:\", scores)\n",
    "print(\"Average R² score:\", np.mean(scores))\n",
    "\n",
    "\n",
    "# 3) Apply a forward stepwise selection\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(X_train.shape, X_test.shape)\n",
    "print(y_train.shape, y_test.shape)\n",
    "\n",
    "# Forward Sequential Feature Selector\n",
    "sfs_forward = SequentialFeatureSelector(\n",
    "    estimator=LinearRegression(),\n",
    "    k_features=\"parsimonious\",\n",
    "    forward=True,\n",
    "    floating=False,\n",
    "    scoring='r2',\n",
    "    cv=5,\n",
    "    verbose=0)\n",
    "\n",
    "sfs_forward.fit(X_train, y_train)\n",
    "\n",
    "print(f\">> Forward SFS:\")\n",
    "print(f\"   Best CV R²      : {sfs_forward.k_score_:.3f}\")\n",
    "print(f\"   Optimal # feats : {len(sfs_forward.k_feature_idx_)}\")\n",
    "print(f\"   Feature names   : {sfs_forward.k_feature_names_}\")\n",
    "\n",
    "\n",
    "# 4) Evaluate the model\n",
    "selected_features = list(sfs_forward.k_feature_names_)\n",
    "\n",
    "X_train_selected = X_train[selected_features]\n",
    "X_test_selected = X_test[selected_features]\n",
    "\n",
    "# Train and evaluate\n",
    "model.fit(X_train_selected, y_train)\n",
    "test_r2 = model.score(X_test_selected, y_test)\n",
    "print(f\"Test R² for the sfs model: {test_r2:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2: LASSO\n",
    "\n",
    "Please implement a Lasso regression model similar to the Ridge model in the [Regularization](2_Regularization) section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal alpha: 20.0\n",
      "\n",
      "Training R²: 0.47908195299121104\n",
      "\n",
      "   Predictor        Beta\n",
      "3      CHits  177.984173\n",
      "6       Hits  101.982447\n",
      "7      HmRun   52.177420\n",
      "10     Walks   41.664953\n",
      "2        RBI    0.000000\n",
      "0      AtBat    0.000000\n",
      "1       Runs    0.000000\n",
      "5    Assists   -0.000000\n",
      "4     CWalks    0.000000\n",
      "8      Years    0.000000\n",
      "9     Errors   -0.000000 \n",
      "\n",
      "Test R²: 0.31479649243077035\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.api as sm \n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LassoCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Data related processing\n",
    "hitters = sm.datasets.get_rdataset(\"Hitters\", \"ISLR\").data\n",
    "hitters_subset = hitters[[\"Salary\", \"AtBat\", \"Runs\",\"RBI\", \"CHits\", \"CAtBat\", \"CRuns\", \"CWalks\", \"Assists\", \"Hits\", \"HmRun\", \"Years\", \"Errors\", \"Walks\"]].copy()\n",
    "hitters_subset = hitters_subset.drop(columns=[\"CRuns\", \"CAtBat\"]) # Remove highly correlated features (see previous session)\n",
    "hitters_subset.dropna(inplace=True) # drop rows containing missing data\n",
    "\n",
    "y = hitters_subset[\"Salary\"]\n",
    "X = hitters_subset.drop(columns=[\"Salary\"])\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "scaler = StandardScaler() # Scale predictors to mean=0 and std=1\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "\n",
    "# Lasso \n",
    "lambda_range = np.linspace(0.001, 20, 100) \n",
    "\n",
    "# Get the optimal lambda\n",
    "lasso_cv = LassoCV(alphas=lambda_range)\n",
    "lasso_cv.fit(X_train_scaled, y_train) \n",
    "\n",
    "print(f\"Optimal alpha: {lasso_cv.alpha_}\\n\")\n",
    "\n",
    "# Get training R²\n",
    "train_score_ridge= lasso_cv.score(X_train_scaled, y_train)\n",
    "print(f\"Training R²: {train_score_ridge}\\n\")\n",
    "\n",
    "# Put the coefficients into a nicely formatted df for visualization\n",
    "coef_table = pd.DataFrame({\n",
    "    'Predictor': X_train.columns,\n",
    "    'Beta': lasso_cv.coef_\n",
    "})\n",
    "\n",
    "coef_table = coef_table.reindex(coef_table['Beta'].abs().sort_values(ascending=False).index)\n",
    "print(coef_table, \"\\n\")\n",
    "\n",
    "\n",
    "test_score_ridge= lasso_cv.score(X_test_scaled, y_test)\n",
    "print(f\"Test R²: {test_score_ridge}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "psy111",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
