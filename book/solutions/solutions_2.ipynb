{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelling Exercises\n",
    "\n",
    "## Exercise 1: Model Selection\n",
    "\n",
    "Today we are working with the `California Housing dataset`, which you are already familiar with, as we previously used it while exploring resampling method.\n",
    "This dataset is based on the 1990 U.S. Census and includes features describing California districts. \n",
    "\n",
    "1) Familiarize yourself with the data\n",
    "    - What kind of features are in the dataset? What is the target?\n",
    "2) Baseline model \n",
    "    - Create a baseline linear regression model using **all** features and evaluate the model through 5-fold cross validation, using R² as the performance metric\n",
    "    - Print the individual and average R²\n",
    "3) Apply a forward stepwise selection to find a simpler suitable model.\n",
    "    - Split the data into 80% training data and 20% testing data (print the shape to confirm it was sucessful)\n",
    "    - Perform a forward stepwise selection with a linear regression model, 5-fold CV, R² score, and `parsimonious` feature selection (refer to [documentation](https://rasbt.github.io/mlxtend/api_subpackages/mlxtend.feature_selection/) for further information)\n",
    "    - Print the best CV R² as well as the chosen features\n",
    "4) Evaluate the model on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R² scores from each fold: [0.54866323 0.46820691 0.55078434 0.53698703 0.66051406]\n",
      "Average R² score: 0.5530311140279571\n",
      "(16512, 8) (4128, 8)\n",
      "(16512,) (4128,)\n",
      ">> Forward SFS:\n",
      "   Best CV R²      : 0.612\n",
      "   Optimal # feats : 7\n",
      "   Feature names   : ('MedInc', 'HouseAge', 'AveRooms', 'AveBedrms', 'AveOccup', 'Latitude', 'Longitude')\n",
      "Test R² for the sfs model: 0.5757\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from mlxtend.feature_selection import SequentialFeatureSelector\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "# 1) Load the California housing dataset\n",
    "data = fetch_california_housing(as_frame=True)\n",
    "\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "\n",
    "# 2) Create baseline model \n",
    "model = LinearRegression()\n",
    "scores = cross_val_score(model, X, y, cv=5, scoring='r2')\n",
    "\n",
    "# Print the results\n",
    "print(\"R² scores from each fold:\", scores)\n",
    "print(\"Average R² score:\", np.mean(scores))\n",
    "\n",
    "\n",
    "# 3) Apply a forward stepwise selection\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(X_train.shape, X_test.shape)\n",
    "print(y_train.shape, y_test.shape)\n",
    "\n",
    "# Forward Sequential Feature Selector\n",
    "sfs_forward = SequentialFeatureSelector(\n",
    "    estimator=LinearRegression(),\n",
    "    k_features=\"parsimonious\",\n",
    "    forward=True,\n",
    "    floating=False,\n",
    "    scoring='r2',\n",
    "    cv=5,\n",
    "    verbose=0)\n",
    "\n",
    "sfs_forward.fit(X_train, y_train)\n",
    "\n",
    "print(f\">> Forward SFS:\")\n",
    "print(f\"   Best CV R²      : {sfs_forward.k_score_:.3f}\")\n",
    "print(f\"   Optimal # feats : {len(sfs_forward.k_feature_idx_)}\")\n",
    "print(f\"   Feature names   : {sfs_forward.k_feature_names_}\")\n",
    "\n",
    "\n",
    "# 4) Evaluate the model\n",
    "selected_features = list(sfs_forward.k_feature_names_)\n",
    "\n",
    "X_train_selected = X_train[selected_features]\n",
    "X_test_selected = X_test[selected_features]\n",
    "\n",
    "# Train and evaluate\n",
    "model.fit(X_train_selected, y_train)\n",
    "test_r2 = model.score(X_test_selected, y_test)\n",
    "print(f\"Test R² for the sfs model: {test_r2:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2: LASSO\n",
    "\n",
    "Please implement a Lasso regression model similar to the Ridge model in the [Regularization](2_Regularization) section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal alpha: 20.0\n",
      "\n",
      "Training R²: 0.47908195299121104\n",
      "\n",
      "   Predictor        Beta\n",
      "3      CHits  177.984173\n",
      "6       Hits  101.982447\n",
      "7      HmRun   52.177420\n",
      "10     Walks   41.664953\n",
      "2        RBI    0.000000\n",
      "0      AtBat    0.000000\n",
      "1       Runs    0.000000\n",
      "5    Assists   -0.000000\n",
      "4     CWalks    0.000000\n",
      "8      Years    0.000000\n",
      "9     Errors   -0.000000 \n",
      "\n",
      "Test R²: 0.31479649243077035\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.api as sm \n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LassoCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Data related processing\n",
    "hitters = sm.datasets.get_rdataset(\"Hitters\", \"ISLR\").data\n",
    "hitters_subset = hitters[[\"Salary\", \"AtBat\", \"Runs\",\"RBI\", \"CHits\", \"CAtBat\", \"CRuns\", \"CWalks\", \"Assists\", \"Hits\", \"HmRun\", \"Years\", \"Errors\", \"Walks\"]].copy()\n",
    "hitters_subset = hitters_subset.drop(columns=[\"CRuns\", \"CAtBat\"]) # Remove highly correlated features (see previous session)\n",
    "hitters_subset.dropna(inplace=True) # drop rows containing missing data\n",
    "\n",
    "y = hitters_subset[\"Salary\"]\n",
    "X = hitters_subset.drop(columns=[\"Salary\"])\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "scaler = StandardScaler() # Scale predictors to mean=0 and std=1\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "\n",
    "# Lasso \n",
    "lambda_range = np.linspace(0.001, 20, 100) \n",
    "\n",
    "# Get the optimal lambda\n",
    "lasso_cv = LassoCV(alphas=lambda_range)\n",
    "lasso_cv.fit(X_train_scaled, y_train) \n",
    "\n",
    "print(f\"Optimal alpha: {lasso_cv.alpha_}\\n\")\n",
    "\n",
    "# Get training R²\n",
    "train_score_ridge= lasso_cv.score(X_train_scaled, y_train)\n",
    "print(f\"Training R²: {train_score_ridge}\\n\")\n",
    "\n",
    "# Put the coefficients into a nicely formatted df for visualization\n",
    "coef_table = pd.DataFrame({\n",
    "    'Predictor': X_train.columns,\n",
    "    'Beta': lasso_cv.coef_\n",
    "})\n",
    "\n",
    "coef_table = coef_table.reindex(coef_table['Beta'].abs().sort_values(ascending=False).index)\n",
    "print(coef_table, \"\\n\")\n",
    "\n",
    "\n",
    "test_score_ridge= lasso_cv.score(X_test_scaled, y_test)\n",
    "print(f\"Test R²: {test_score_ridge}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3: GAMs (1)\n",
    "\n",
    "Objective: Understand how the number of basis functions (df) and the polynomial degree (degree) affect the flexibility of a spline and the resulting fit in a Generalized Additive Model.\n",
    "\n",
    "1. Use the diabetes dataset and focus on the relationship between `bmi` and `target`.\n",
    "2. We want to test different combinations of parameters. For the dfs, please use 4, 6, 12. For the degree, please use 2 and 3 (quadratic and cubic).\n",
    "3. Fit the GAMs for each parameter combination. The resulting models will be plotted automatically for visual comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_diabetes\n",
    "from statsmodels.gam.api import GLMGam, BSplines\n",
    "\n",
    "# 1. Get bmi as x and the target as y\n",
    "data = load_diabetes(as_frame=True)\n",
    "x = data.data[['bmi']]\n",
    "y = data.target\n",
    "\n",
    "# 2. Define possible parameters\n",
    "df_values = [4, 6, 12]\n",
    "degree_values = [2, 3]\n",
    "\n",
    "# 3. PLot partial effect for each combination of df and degree\n",
    "fig, axes = plt.subplots(len(df_values), len(degree_values), figsize=(15, 10), sharey=True)\n",
    "\n",
    "for i, df_val in enumerate(df_values):\n",
    "    for j, deg_val in enumerate(degree_values):\n",
    "        bs = BSplines(x, df=df_val, degree=deg_val)\n",
    "        gam = GLMGam(y, smoother=bs)\n",
    "        res = gam.fit()\n",
    "\n",
    "        res.plot_partial(0, cpr=True, ax=axes[i, j])\n",
    "        axes[i, j].set_title(f'B-spline: df={df_val}, degree={deg_val}')\n",
    "        axes[i, j].set_xlabel('BMI')\n",
    "        axes[i, j].set_ylabel('Effect')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4: GAMs (2)\n",
    "\n",
    "We now use the [wage](https://islp.readthedocs.io/en/latest/datasets/Wage.html) dataset, which contains income information for a group of workers, along with demographic and employment-related features such as age, education, marital status, and job class.\n",
    "\n",
    "1) Explore the dataset\n",
    "    - Which variables are numeric?\n",
    "    - Which ones are categorical?\n",
    "\n",
    "2) Fit a GAM predicting `wage` from `age`, `year`, `education`, `jobclass`, and `maritl`\n",
    "\n",
    "Note: For categorical features we use a one-hot encoding with `pd.get_dummies()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from ISLP import load_data\n",
    "from statsmodels.gam.api import GLMGam, BSplines\n",
    "\n",
    "# Load data\n",
    "Wage = load_data('Wage')\n",
    "\n",
    "# Continuous features\n",
    "spline_features = ['age', 'year']\n",
    "X_spline = Wage[spline_features]\n",
    "\n",
    "# Categorical features — one-hot encode\n",
    "categoricals = ['education', 'jobclass', 'maritl']\n",
    "X_cat = pd.get_dummies(Wage[categoricals], drop_first=True)\n",
    "\n",
    "# Outcome\n",
    "y = Wage['wage']\n",
    "\n",
    "# Create BSpline basis\n",
    "bs = BSplines(X_spline, df=[6]*len(spline_features), degree=[3]*len(spline_features))\n",
    "\n",
    "# Fit GAM\n",
    "gam = GLMGam(y, exog=X_cat, smoother=bs)\n",
    "res = gam.fit()\n",
    "\n",
    "print(res.summary())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "psy111",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
