{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5930baa2",
   "metadata": {},
   "source": [
    "# <i class=\"fa-solid fa-dumbbell\"></i> Exercises\n",
    "\n",
    "Please fill the missing code pieces as indicated by the `...`. The imports are always provided at the top of the code chunks. This should give you a hint for which functions/classes to use."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc0081c1",
   "metadata": {},
   "source": [
    "## Exercise 1: Model Selection\n",
    "\n",
    "Today we are working with the `California Housing dataset`, which you are already familiar with, as we previously used it while exploring resampling method.\n",
    "This dataset is based on the 1990 U.S. Census and includes features describing California districts. \n",
    "\n",
    "1) Familiarize yourself with the data\n",
    "    - What kind of features are in the dataset? What is the target?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19454dd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "\n",
    "data = fetch_california_housing(as_frame=True)\n",
    "\n",
    "X = data.data\n",
    "y = data.target"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7d510db",
   "metadata": {},
   "source": [
    "2) Baseline model \n",
    "    - Create a baseline linear regression model using **all** features and evaluate the model through 5-fold cross validation, using R² as the performance metric\n",
    "    - Print the individual and average R²"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51272717",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import numpy as np\n",
    "\n",
    "# Regression model\n",
    "model = ...\n",
    "scores = ...\n",
    "\n",
    "# Print the results\n",
    "print(\"R² scores from each fold:\", scores)\n",
    "print(\"Average R² score:\", np.mean(scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3648e8d",
   "metadata": {},
   "source": [
    "3) Apply a forward stepwise selection to find a simpler suitable model.\n",
    "    - Split the data into 80% training data and 20% testing data (print the shape to confirm it was sucessful)\n",
    "    - Perform a forward stepwise selection with a linear regression model, 5-fold CV, R² score, and `parsimonious` feature selection (refer to [documentation](https://rasbt.github.io/mlxtend/api_subpackages/mlxtend.feature_selection/) for further information)\n",
    "    - Print the best CV R² as well as the chosen features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "499b190e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlxtend.feature_selection import SequentialFeatureSelector\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = ...\n",
    "\n",
    "print(X_train.shape, X_test.shape)\n",
    "print(y_train.shape, y_test.shape)\n",
    "\n",
    "# Forward Sequential Feature Selector\n",
    "sfs_forward = ...\n",
    "\n",
    "sfs_forward.fit(...)\n",
    "\n",
    "print(f\">> Forward SFS:\")\n",
    "print(f\"   Best CV R²      : {sfs_forward.k_score_:.3f}\")\n",
    "print(f\"   Optimal # feats : {len(sfs_forward.k_feature_idx_)}\")\n",
    "print(f\"   Feature names   : {sfs_forward.k_feature_names_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d10c2d4d",
   "metadata": {},
   "source": [
    "4) Evaluate the model on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca5d168b",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_features = list(sfs_forward.k_feature_names_)\n",
    "\n",
    "X_train_selected = X_train[selected_features]\n",
    "X_test_selected = X_test[selected_features]\n",
    "\n",
    "# Train and evaluate\n",
    "model.fit(...)\n",
    "test_r2 = model.score(...)\n",
    "print(f\"Test R² for the sfs model: {test_r2:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0e29057",
   "metadata": {},
   "source": [
    "## Exercise 2: LASSO\n",
    "\n",
    "Please implement a Lasso regression model similar to the Ridge model in the [Regularization](2_Regularization) section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0bfce1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.api as sm \n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LassoCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Data related processing\n",
    "hitters = sm.datasets.get_rdataset(\"Hitters\", \"ISLR\").data\n",
    "hitters_subset = hitters[[\"Salary\", \"AtBat\", \"Runs\",\"RBI\", \"CHits\", \"CAtBat\", \"CRuns\", \"CWalks\", \"Assists\", \"Hits\", \"HmRun\", \"Years\", \"Errors\", \"Walks\"]].copy()\n",
    "hitters_subset = hitters_subset.drop(columns=[\"CRuns\", \"CAtBat\"]) # Remove highly correlated features (see previous session)\n",
    "hitters_subset.dropna(inplace=True) # drop rows containing missing data\n",
    "\n",
    "y = hitters_subset[\"Salary\"]\n",
    "X = hitters_subset.drop(columns=[\"Salary\"])\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Scale predictors to mean=0 and std=1\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# TODO: Implement Lasso \n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b9cafe4",
   "metadata": {},
   "source": [
    "## Exercise 3: GAM\n",
    "The dataset we are using for the exercise is the [Diabetes Dataset](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_diabetes.html), which you already know from the theoretical part of the seminar. \n",
    "\n",
    "It contains: \n",
    "- 10 baseline features measured at the beginning of the study:\n",
    "    - age, sex, Body Mass Index (BMI), average blood pressure, six blood serum measurements (e.g. cholesterol, blood sugar, etc.)\n",
    "- Target variable: A quantitative measure of disease progression one year after the baseline measurements were taken.\n",
    "\n",
    "Let's prepare the data for further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e03ef40",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# load data\n",
    "diabetes = datasets.load_diabetes(as_frame=True)\n",
    "\n",
    "# TODO: define feature and target\n",
    "X= diabetes.data\n",
    "y= diabetes.target\n",
    "\n",
    "# TODO: split the data \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9708d565",
   "metadata": {},
   "source": [
    "Let`s also again plot the target and features to see which model may describe the relationship the best."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b637ee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn import datasets\n",
    "\n",
    "# Combine into a single DataFrame for easy plotting\n",
    "df = X.copy()\n",
    "df['target'] = y\n",
    "features_to_plot = X.columns\n",
    "# Set up the plot grid\n",
    "num_features = len(features_to_plot)\n",
    "cols = 5\n",
    "rows = (num_features + cols - 1) // cols\n",
    "fig, axes = plt.subplots(rows, cols, figsize=(15, 4 * rows))\n",
    "axes = axes.flatten()\n",
    "# Plot each feature (except 'sex') against the target\n",
    "for i, col in enumerate(features_to_plot):\n",
    "    sns.regplot(x=col, y='target', data=df, ax=axes[i], scatter_kws={'s': 10}, line_kws={'color': 'red'})\n",
    "    axes[i].set_title(f\"{col} vs target\")\n",
    "# Hide any unused subplots\n",
    "for j in range(i + 1, len(axes)):\n",
    "    fig.delaxes(axes[j])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "418a8d10",
   "metadata": {},
   "source": [
    "In the session, we noticed that the model we fitted did not perform well on our data — even a simple linear model yielded better results.\n",
    "\n",
    "1. Your task is to improve the model's performance on unseen data. Use the feature-target plots to determine the most appropriate modeling approach (linear, smooth, or categorical) for each feature. Select the type of model based on the relationship observed in these plots. Remember, that you could also adjust the degrees of freedom.\n",
    "\n",
    "    Can you build a model that outperforms the simple linear model, which achieved an R² of 0.48? Or is the linear model the best fit for our data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6444ec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from patsy import dmatrix\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "# combine into one  datafra,e\n",
    "train_data= X_train.copy()\n",
    "train_data[\"target\"]= y_train\n",
    "\n",
    "test_data = X_test.copy()\n",
    "test_data[\"target\"] = y_test\n",
    "\n",
    "\n",
    "# TODO: define formula\n",
    "formula = (\n",
    "    \"bs(age, df=6) + \"           \n",
    "    \"C(sex) + \"                 \n",
    "    \"bs(bmi, df=6)+ \"\n",
    "    \"bs(bp, df=3) + \"\n",
    "    \"bs(s1, df=6) + \"\n",
    "    \"bs(s2, df=6) + \"\n",
    "    \"bs(s3, df=6) + \"\n",
    "    \"bs(s4, df=6) + \"\n",
    "    \"s5 + \"                      \n",
    "    \"bs(s6, df=6)\"              \n",
    ")\n",
    "\n",
    "\n",
    "# TODO: Build design matrix from training data\n",
    "X_train_design = dmatrix(formula, data=train_data, return_type=\"dataframe\")\n",
    "X_test_design = dmatrix(formula, data=test_data, return_type=\"dataframe\")\n",
    "\n",
    "#Fit model\n",
    "model = LinearRegression()\n",
    "model.fit(X_train_design, y_train)\n",
    "\n",
    "# Predictions & Evaluation\n",
    "y_pred = model.predict(X_test_design)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "print(f\"R² on test data: {r2:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11a379c7",
   "metadata": {},
   "source": [
    "## Exercise 4: GAM \n",
    "In this exercise, we’ll continue exploring Generalized Additive Models (GAMs).\n",
    "Our new dataset - the [wage](https://islp.readthedocs.io/en/latest/datasets/Wage.html) dataset - contains information on **wage income** for a group of workers, along with demographic and employment-related features such as age, education, marital status, and job class. Unlike our earlier examples, this dataset includes a mix of numeric and categorical variables, and the categorical variables are **not dummy-coded** — they are stored as strings or categories.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b4dd866",
   "metadata": {},
   "source": [
    "1) Explore the dataset\n",
    "    - Which variables are numeric?\n",
    "    - Which ones are categorical?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7307bb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ISLP import load_data\n",
    "import pandas as pd\n",
    "\n",
    "# Load Wage dataset\n",
    "Wage = load_data('Wage')\n",
    "\n",
    "# Define X and y\n",
    "X = Wage[[\"age\", \"jobclass\", \"race\", \"education\", \"year\"]]  # In this case we are only using 5 features\n",
    "y = Wage['wage']                                            # Target\n",
    "\n",
    "# TODO: Explore the features.\n",
    "print(X.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aa79fb0",
   "metadata": {},
   "source": [
    "2) Plot the features against the target variable *wage*. Focus especially on features where you're not sure whether the relationship is linear or nonlinear. You do not need to plot categorical variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba10df3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# TODO: Plot the feature \n",
    "# Combine X and y for plotting\n",
    "data = X.copy()\n",
    "data['wage'] = y\n",
    "\n",
    "# Define only the features you want to plot\n",
    "features_to_plot = ['age', 'year']\n",
    "\n",
    "# Plot each selected feature\n",
    "for col in features_to_plot:\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    sns.regplot(x=col, y='wage', data=data, scatter_kws={'s': 10}, line_kws={'color': 'red'})\n",
    "    plt.title(f\"{col} vs wage\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41cb035a",
   "metadata": {},
   "source": [
    "3) Fit a Generalized Additive Model (GAM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fc08972",
   "metadata": {},
   "outputs": [],
   "source": [
    "from patsy import dmatrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "\n",
    "# TODO: Train-Test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "\n",
    "# TODO: Combine into one dataframe\n",
    "train_data = X_train.copy()\n",
    "train_data[\"target\"] = y_train\n",
    "\n",
    "test_data = X_test.copy()\n",
    "test_data[\"target\"] = y_test\n",
    "\n",
    "#  TODO: Generate formula\n",
    "formula = (\n",
    "    \"bs(age, df=6) + \"            # smooth\n",
    "    \"bs(year, df=6) + \"           # smooth\n",
    "    \"C(race) + \"                  # categorical\n",
    "    \"C(education) + \"             # categorical\n",
    "    \"C(jobclass)\"                 # categorical\n",
    ")\n",
    "\n",
    "# TODO: Design Matrix\n",
    "X_train_design = dmatrix(formula, data=train_data, return_type=\"dataframe\")\n",
    "X_test_design = dmatrix(formula, data=test_data, return_type=\"dataframe\")\n",
    "\n",
    "# TODO: Fit the model\n",
    "model = LinearRegression()\n",
    "model.fit(X_train_design, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27bf99a6",
   "metadata": {},
   "source": [
    "4) Evaluate model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76bf3bf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Predictions & Evaluation \n",
    "y_pred = model.predict(X_test_design)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "\n",
    "print(f\"R² on test data: {r2:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d28d298",
   "metadata": {},
   "source": [
    "AN MICHA: Warum performen die models immer so schlecht?! "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d839a794",
   "metadata": {},
   "source": [
    "## Exercise 5: KNN\n",
    "For this exercise we are using the **Wine** dataset from `sklearn.datasets`. It is a classic classification dataset containing the chemical analysis of wines grown in the same region of Italy but derived from three different cultivars (wine classes). Let's have a look at the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a0b6f0d",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f5252f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_wine\n",
    "data = load_wine(as_frame=True)\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "#print(X)\n",
    "print(X.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb43de5a",
   "metadata": {},
   "source": [
    "The dataset contains 13 features in total. To simplify our task and better visualize the classification process, let's select only **two features** and use them to build a K-Nearest Neighbors (KNN) classifier.\n",
    "\n",
    "\n",
    "1) Choose any two features from the dataset and assign them to a new variable X. Keep in mind that using only two out of thirteen features means we’re working with just 15% of the available information — so while the model will be easy to interpret and plot, it likely won’t achieve the best possible performance!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21f5df24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO- Choose 2 features and define them to your new X!\n",
    "# X= X[[\"alcohol\", \"flavanoids\"]]\n",
    "X= X[[\"alcohol\", \"magnesium\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "994c9dd3",
   "metadata": {},
   "source": [
    "2) Visualize the chosen features with a scatterplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bb55ba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# TODO: Complete the code to get a scatterplot showing your 2 features\n",
    "# Create scatter plot\n",
    "plt.figure(figsize=(8, 6))\n",
    "scatter = plt.scatter(\n",
    "    X['alcohol'], X['magnesium'], c=y, cmap='viridis', edgecolor='k'\n",
    ")\n",
    "\n",
    "# Axis labels and title\n",
    "plt.xlabel(\"Alcohol\")\n",
    "plt.ylabel(\"Flavanoids\")\n",
    "plt.title(\"Wine Dataset: Alcohol vs Magnesium\")\n",
    "\n",
    "# Add legend\n",
    "classes = list(set(y))\n",
    "handles = scatter.legend_elements()[0]\n",
    "labels = [f\"Class {c}\" for c in classes]\n",
    "plt.legend(handles=handles, labels=labels, title=\"Wine Class\", loc=\"lower right\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98ab4e5c",
   "metadata": {},
   "source": [
    "Take a moment to explore the data you’ve just visualized. Does it appear to be clearly separated into distinct classes, like the famous Iris dataset? Or is there a lot of overlap between the groups? \n",
    "\n",
    "Based on what you see, make a guess: \n",
    "- Do you expect that a high or low value of k will work best for KNN in this case? \n",
    "- And given that you’re only using two features, do you think the model will achieve high accuracy, or might performance suffer because of limited information?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3e6cd60",
   "metadata": {},
   "source": [
    "3) Split the data set using `train_test_split` and standardize it. Do you remember, which one to do first?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4488878",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# TODO: Split the data into test and trainings data set and standardize\n",
    "# Tip: Use strtify=y in train_test_split to  ensures that all classes are represented proportionally in both \n",
    "# the training and test sets — which is especially important in multiclass classification like the Wine dataset.\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42, stratify=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "574244f0",
   "metadata": {},
   "source": [
    "4) Identify the best k for your data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20ff2e03",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# TODO: Choose a range for k\n",
    "k_range= list(range(1, 46))\n",
    "\n",
    "\n",
    "# variable to store the accuracy scores in loop\n",
    "scores= []\n",
    "\n",
    "# loop trough the range of k using cross validation\n",
    "for k in k_range:\n",
    "    knn = KNeighborsClassifier(n_neighbors=k)\n",
    "    score = cross_val_score(knn, X_scaled, y, cv=5)      # get scores for each k\n",
    "    scores.append(np.mean(score))                 # append mean score to list\n",
    "\n",
    "# TODO: plot accurcay angainst the range of k\n",
    "sns.lineplot(x = k_range, y = scores, marker = 'o')\n",
    "plt.xlabel(\"K Values\")\n",
    "plt.ylabel(\"Accuracy Score\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0f4af4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Identify the best k\n",
    "best_index = np.argmax(scores)\n",
    "# getting best k\n",
    "best_k = k_range[best_index]\n",
    "# getting accuracy of best k\n",
    "best_score = scores[best_index]\n",
    "\n",
    "print(f\"Best k: {best_k}\")\n",
    "print(f\"Accuracy with best k: {best_score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef6754cc",
   "metadata": {},
   "source": [
    "5) Finally train the model using the best k and evalute the model performane on unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1f07e94",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "\n",
    "# TODO:  Train the model using training data\n",
    "knn = KNeighborsClassifier(n_neighbors=31)\n",
    "knn.fit(X_train, y_train)\n",
    "\n",
    "#The model is now trained using the training data. Next, we can use it to make predictions on the test set. \n",
    "\n",
    "# predict the feature-category with the trained model\n",
    "y_pred = knn.predict(X_test)\n",
    "\n",
    "# TODO: check accuracy\n",
    "accuracy_score(y_test, y_pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "psy111",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
