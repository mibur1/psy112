{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5930baa2",
   "metadata": {},
   "source": [
    "# <i class=\"fa-solid fa-dumbbell\"></i> Exercises\n",
    "\n",
    "Please fill the missing code pieces as indicated by the `...`. The imports are always provided at the top of the code chunks. This should give you a hint for which functions/classes to use."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc0081c1",
   "metadata": {},
   "source": [
    "## Exercise 1: Model Selection\n",
    "\n",
    "Today we are working with the `California Housing dataset`, which you are already familiar with, as we previously used it while exploring resampling method.\n",
    "This dataset is based on the 1990 U.S. Census and includes features describing California districts. \n",
    "\n",
    "1) Familiarize yourself with the data\n",
    "    - What kind of features are in the dataset? What is the target?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19454dd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "\n",
    "data = fetch_california_housing(as_frame=True)\n",
    "\n",
    "X = data.data\n",
    "y = data.target"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7d510db",
   "metadata": {},
   "source": [
    "2) Baseline model \n",
    "    - Create a baseline linear regression model using **all** features and evaluate the model through 5-fold cross validation, using R² as the performance metric\n",
    "    - Print the individual and average R²"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51272717",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import numpy as np\n",
    "\n",
    "# Regression model\n",
    "model = ...\n",
    "scores = ...\n",
    "\n",
    "# Print the results\n",
    "print(\"R² scores from each fold:\", scores)\n",
    "print(\"Average R² score:\", np.mean(scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3648e8d",
   "metadata": {},
   "source": [
    "3) Apply a forward stepwise selection to find a simpler suitable model.\n",
    "    - Split the data into 80% training data and 20% testing data (print the shape to confirm it was sucessful)\n",
    "    - Perform a forward stepwise selection with a linear regression model, 5-fold CV, R² score, and `parsimonious` feature selection (refer to [documentation](https://rasbt.github.io/mlxtend/api_subpackages/mlxtend.feature_selection/) for further information)\n",
    "    - Print the best CV R² as well as the chosen features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "499b190e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlxtend.feature_selection import SequentialFeatureSelector\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = ...\n",
    "\n",
    "print(X_train.shape, X_test.shape)\n",
    "print(y_train.shape, y_test.shape)\n",
    "\n",
    "# Forward Sequential Feature Selector\n",
    "sfs_forward = ...\n",
    "\n",
    "sfs_forward.fit(...)\n",
    "\n",
    "print(f\">> Forward SFS:\")\n",
    "print(f\"   Best CV R²      : {sfs_forward.k_score_:.3f}\")\n",
    "print(f\"   Optimal # feats : {len(sfs_forward.k_feature_idx_)}\")\n",
    "print(f\"   Feature names   : {sfs_forward.k_feature_names_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d10c2d4d",
   "metadata": {},
   "source": [
    "4) Evaluate the model on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca5d168b",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_features = list(sfs_forward.k_feature_names_)\n",
    "\n",
    "X_train_selected = X_train[selected_features]\n",
    "X_test_selected = X_test[selected_features]\n",
    "\n",
    "# Train and evaluate\n",
    "model.fit(...)\n",
    "test_r2 = model.score(...)\n",
    "print(f\"Test R² for the sfs model: {test_r2:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0e29057",
   "metadata": {},
   "source": [
    "## Exercise 2: LASSO\n",
    "\n",
    "Please implement a Lasso regression model similar to the Ridge model in the [Regularization](2_Regularization) section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0bfce1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.api as sm \n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LassoCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Data related processing\n",
    "hitters = sm.datasets.get_rdataset(\"Hitters\", \"ISLR\").data\n",
    "hitters_subset = hitters[[\"Salary\", \"AtBat\", \"Runs\",\"RBI\", \"CHits\", \"CAtBat\", \"CRuns\", \"CWalks\", \"Assists\", \"Hits\", \"HmRun\", \"Years\", \"Errors\", \"Walks\"]].copy()\n",
    "hitters_subset = hitters_subset.drop(columns=[\"CRuns\", \"CAtBat\"]) # Remove highly correlated features (see previous session)\n",
    "hitters_subset.dropna(inplace=True) # drop rows containing missing data\n",
    "\n",
    "y = hitters_subset[\"Salary\"]\n",
    "X = hitters_subset.drop(columns=[\"Salary\"])\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Scale predictors to mean=0 and std=1\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# TODO: Implement Lasso \n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b9cafe4",
   "metadata": {},
   "source": [
    "## Exercise 3: GAMs (2)\n",
    "\n",
    "Objective: Understand how the number of basis functions (df) and the polynomial degree (degree) affect the flexibility of a spline and the resulting fit in a Generalized Additive Model.\n",
    "\n",
    "1. Use the diabetes dataset and focus on the relationship between `bmi` and `target`.\n",
    "2. We want to test different combinations of parameters. For the dfs, please use 4, 6, 12. For the degree, please use 2 and 3 (quadratic and cubic).\n",
    "3. Fit the GAMs for each parameter combination. The resulting models will be plotted automatically for visual comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed2f0d41",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_diabetes\n",
    "from statsmodels.gam.api import GLMGam, BSplines\n",
    "\n",
    "\n",
    "# TODO: 1. Get bmi as x and the target as y\n",
    "data = load_diabetes(as_frame=True)\n",
    "x = ...\n",
    "y = ...\n",
    "\n",
    "# TODO: 2. Define possible parameters\n",
    "df_values = ...\n",
    "degree_values = ...\n",
    "\n",
    "# TODO: 3. Plot partial effect for each combination of df and degree\n",
    "fig, axes = plt.subplots(len(df_values), len(degree_values), figsize=(15, 10), sharey=True)\n",
    "\n",
    "for i, df_val in enumerate(df_values):\n",
    "    for j, deg_val in enumerate(degree_values):\n",
    "        bs = BSplines(...)\n",
    "        gam = GLMGam(...)\n",
    "        res = gam.fit()\n",
    "\n",
    "        res.plot_partial(0, cpr=True, ax=axes[i, j])\n",
    "        axes[i, j].set_title(f'B-spline: df={df_val}, degree={deg_val}')\n",
    "        axes[i, j].set_xlabel('BMI')\n",
    "        axes[i, j].set_ylabel('Effect')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11a379c7",
   "metadata": {},
   "source": [
    "## Exercise 4: GAMs (2)\n",
    "\n",
    "We now use the [wage](https://islp.readthedocs.io/en/latest/datasets/Wage.html) dataset, which contains income information for a group of workers, along with demographic and employment-related features such as age, education, marital status, and job class.\n",
    "\n",
    "1) Explore the dataset\n",
    "    - Which variables are numeric?\n",
    "    - Which ones are categorical?\n",
    "\n",
    "2) Fit a GAM predicting `wage` from `age`, `year`, `education`, `jobclass`, and `maritl`\n",
    "\n",
    "Note: For categorical features we use a one-hot encoding with `pd.get_dummies()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ec422ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from ISLP import load_data\n",
    "from statsmodels.gam.api import GLMGam, BSplines\n",
    "\n",
    "# Load data\n",
    "Wage = load_data('Wage')\n",
    "\n",
    "# Continuous features\n",
    "smoooth_features = ...\n",
    "X_spline = Wage[smoooth_features]\n",
    "\n",
    "# Categorical features — one-hot encode\n",
    "categoricals = ...\n",
    "X_cat = pd.get_dummies(Wage[categoricals], drop_first=True)\n",
    "\n",
    "# Outcome\n",
    "y = ...\n",
    "\n",
    "# Create BSpline basis\n",
    "bs = BSplines(...)\n",
    "\n",
    "# Fit GAM\n",
    "gam = GLMGam(...)\n",
    "res = gam.fit()\n",
    "\n",
    "print(res.summary())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "psy111",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
