{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5930baa2",
   "metadata": {},
   "source": [
    "# <i class=\"fa-solid fa-dumbbell\"></i> Exercises\n",
    "\n",
    "Please fill the missing code pieces as indicated by the `...`. The imports are always provided at the top of the code chunks. This should give you a hint for which functions/classes to use."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc0081c1",
   "metadata": {},
   "source": [
    "## Exercise 1: Model Selection\n",
    "\n",
    "Today we are working with the `California Housing dataset`, which you are already familiar with, as we previously used it while exploring resampling method.\n",
    "This dataset is based on the 1990 U.S. Census and includes features describing California districts. \n",
    "\n",
    "1) Familiarize yourself with the data\n",
    "    - What kind of features are in the dataset? What is the target?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19454dd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "\n",
    "data = fetch_california_housing(as_frame=True)\n",
    "\n",
    "X = data.data\n",
    "y = data.target"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7d510db",
   "metadata": {},
   "source": [
    "2) Baseline model \n",
    "    - Create a baseline linear regression model using **all** features and evaluate the model through 5-fold cross validation, using R² as the performance metric\n",
    "    - Print the individual and average R²"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51272717",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import numpy as np\n",
    "\n",
    "# Regression model\n",
    "model = ...\n",
    "scores = ...\n",
    "\n",
    "# Print the results\n",
    "print(\"R² scores from each fold:\", scores)\n",
    "print(\"Average R² score:\", np.mean(scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3648e8d",
   "metadata": {},
   "source": [
    "3) Apply a forward stepwise selection to find a simpler suitable model.\n",
    "    - Split the data into 80% training data and 20% testing data (print the shape to confirm it was sucessful)\n",
    "    - Perform a forward stepwise selection with a linear regression model, 5-fold CV, R² score, and `parsimonious` feature selection (refer to [documentation](https://rasbt.github.io/mlxtend/api_subpackages/mlxtend.feature_selection/) for further information)\n",
    "    - Print the best CV R² as well as the chosen features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "499b190e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlxtend.feature_selection import SequentialFeatureSelector\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = ...\n",
    "\n",
    "print(X_train.shape, X_test.shape)\n",
    "print(y_train.shape, y_test.shape)\n",
    "\n",
    "# Forward Sequential Feature Selector\n",
    "sfs_forward = ...\n",
    "\n",
    "sfs_forward.fit(...)\n",
    "\n",
    "print(f\">> Forward SFS:\")\n",
    "print(f\"   Best CV R²      : {sfs_forward.k_score_:.3f}\")\n",
    "print(f\"   Optimal # feats : {len(sfs_forward.k_feature_idx_)}\")\n",
    "print(f\"   Feature names   : {sfs_forward.k_feature_names_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d10c2d4d",
   "metadata": {},
   "source": [
    "4) Evaluate the model on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca5d168b",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_features = list(sfs_forward.k_feature_names_)\n",
    "\n",
    "X_train_selected = X_train[selected_features]\n",
    "X_test_selected = X_test[selected_features]\n",
    "\n",
    "# Train and evaluate\n",
    "model.fit(...)\n",
    "test_r2 = model.score(...)\n",
    "print(f\"Test R² for the sfs model: {test_r2:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0e29057",
   "metadata": {},
   "source": [
    "## Exercise 2: LASSO\n",
    "\n",
    "Please implement a Lasso regression model similar to the Ridge model in the [Regularization](2_Regularization) section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0bfce1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.api as sm \n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LassoCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Data related processing\n",
    "hitters = sm.datasets.get_rdataset(\"Hitters\", \"ISLR\").data\n",
    "hitters_subset = hitters[[\"Salary\", \"AtBat\", \"Runs\",\"RBI\", \"CHits\", \"CAtBat\", \"CRuns\", \"CWalks\", \"Assists\", \"Hits\", \"HmRun\", \"Years\", \"Errors\", \"Walks\"]].copy()\n",
    "hitters_subset = hitters_subset.drop(columns=[\"CRuns\", \"CAtBat\"]) # Remove highly correlated features (see previous session)\n",
    "hitters_subset.dropna(inplace=True) # drop rows containing missing data\n",
    "\n",
    "y = hitters_subset[\"Salary\"]\n",
    "X = hitters_subset.drop(columns=[\"Salary\"])\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Scale predictors to mean=0 and std=1\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# TODO: Implement Lasso \n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b9cafe4",
   "metadata": {},
   "source": [
    "## Exercise 3: GAMs (1)\n",
    "\n",
    "Objective: Understand how the number of basis functions (df) and the polynomial degree (degree) affect the flexibility of a spline and the resulting fit in a Generalized Additive Model.\n",
    "\n",
    "1. Use the diabetes dataset and focus on the relationship between `bmi` and `target`.\n",
    "2. We want to test different combinations of parameters. For the dfs, please use 4, 6, 12. For the degree, please use 2 and 3 (quadratic and cubic).\n",
    "3. Fit the GAMs for each parameter combination. The resulting models will be plotted automatically for visual comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed2f0d41",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_diabetes\n",
    "from statsmodels.gam.api import GLMGam, BSplines\n",
    "\n",
    "\n",
    "# TODO: 1. Get bmi as x and the target as y\n",
    "data = load_diabetes(as_frame=True)\n",
    "x = ...\n",
    "y = ...\n",
    "\n",
    "# TODO: 2. Define possible parameters\n",
    "df_values = ...\n",
    "degree_values = ...\n",
    "\n",
    "# TODO: 3. Plot partial effect for each combination of df and degree\n",
    "fig, axes = plt.subplots(len(df_values), len(degree_values), figsize=(15, 10), sharey=True)\n",
    "\n",
    "for i, df_val in enumerate(df_values):\n",
    "    for j, deg_val in enumerate(degree_values):\n",
    "        bs = BSplines(...)\n",
    "        gam = GLMGam(...)\n",
    "        res = gam.fit()\n",
    "\n",
    "        res.plot_partial(0, cpr=True, ax=axes[i, j])\n",
    "        axes[i, j].set_title(f'B-spline: df={df_val}, degree={deg_val}')\n",
    "        axes[i, j].set_xlabel('BMI')\n",
    "        axes[i, j].set_ylabel('Effect')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11a379c7",
   "metadata": {},
   "source": [
    "## Exercise 4: GAMs (2)\n",
    "\n",
    "We now use the [wage](https://islp.readthedocs.io/en/latest/datasets/Wage.html) dataset, which contains income information for a group of workers, along with demographic and employment-related features such as age, education, marital status, and job class.\n",
    "\n",
    "1) Explore the dataset\n",
    "    - Which variables are numeric?\n",
    "    - Which ones are categorical?\n",
    "\n",
    "2) Fit a GAM predicting `wage` from `age`, `year`, `education`, `jobclass`, and `maritl`\n",
    "\n",
    "Note: For categorical features we use a one-hot encoding with `pd.get_dummies()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ec422ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from ISLP import load_data\n",
    "from statsmodels.gam.api import GLMGam, BSplines\n",
    "\n",
    "# Load data\n",
    "Wage = load_data('Wage')\n",
    "\n",
    "# Continuous features\n",
    "smoooth_features = ...\n",
    "X_spline = Wage[smoooth_features]\n",
    "\n",
    "# Categorical features — one-hot encode\n",
    "categoricals = ...\n",
    "X_cat = pd.get_dummies(Wage[categoricals], drop_first=True)\n",
    "\n",
    "# Outcome\n",
    "y = ...\n",
    "\n",
    "# Create BSpline basis\n",
    "bs = BSplines(...)\n",
    "\n",
    "# Fit GAM\n",
    "gam = GLMGam(...)\n",
    "res = gam.fit()\n",
    "\n",
    "print(res.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72059029",
   "metadata": {},
   "source": [
    "## Exercise 5: KNN\n",
    "\n",
    "You will implement a K-Nearest Neighbours (KNN) classifier to predict whether a patient is likely to have malignant or benign breast cancer based on several features. The data is already loaded for you, but please have a look a the [documentation](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_breast_cancer.html) to quickly refresh you memory about the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3af29c10",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# Load the data\n",
    "data = load_breast_cancer()\n",
    "X, y = data.data, data.target\n",
    "\n",
    "# Create a DataFrame for easier inspection and manipulation\n",
    "df = pd.DataFrame(X, columns=data.feature_names)\n",
    "df['target'] = y\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f987610",
   "metadata": {},
   "source": [
    "Please implement the following:\n",
    "\n",
    "1. Subset the dataframe to use `mean area`, `mean radius`, and `mean smoothness` as features (X), and `target` as the target (y)\n",
    "2. Scale the predictors to mean 0 and variance 1\n",
    "3. Split the data into a training and a testing set (70/30)\n",
    "4. Train a kNN classifier with $k=5$\n",
    "5. Evaluate the performance for the testing set. Please use the `accuracy_score()` as well as the `classification_report()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7003b5df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: 1. Select features and target\n",
    "X = ...\n",
    "y = ...\n",
    "\n",
    "# TODO: 2. Scale the features\n",
    "scaler = ...\n",
    "X_scaled = ...\n",
    "\n",
    "# TODO: 3. Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(...)\n",
    "\n",
    "# 4. Perform KNN classification\n",
    "knn = KNeighborsClassifier(...)\n",
    "knn.fit(...)\n",
    "\n",
    "# TODO: 5. Get predictions\n",
    "y_pred = knn.predict(...)\n",
    "\n",
    "print(\"Accuracy:\", accuracy_score(...))\n",
    "print(\"\\nClassification Report:\\n\", classification_report(...))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cae50387",
   "metadata": {},
   "source": [
    "The classification model from the previous step has two main limitations:\n",
    "\n",
    "1. It is trained and evaluated on a single data split\n",
    "2. It uses a single $k$ even though we do not know if it is optimal\n",
    "\n",
    "Please do the following:\n",
    "\n",
    "1. Implement 5-fold cross validation\n",
    "2. Train models for $k$ ranging from 1 to 200 and plot the mean accuracy over all folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6258cf70",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "import seaborn as sns\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "\n",
    "k_values = range(1, 200)\n",
    "mean_accuracies = []\n",
    "\n",
    "# 5-fold cross-validation for different k values\n",
    "for k in k_values:\n",
    "    knn = KNeighborsClassifier(...)\n",
    "    scores = cross_val_score(...)\n",
    "    mean_accuracies.append(...)\n",
    "\n",
    "# Plot\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "sns.lineplot(x=..., y=..., linestyle='-', ax=ax)\n",
    "ax.set(xlabel=..., ylabel=..., title=...);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09bc67b0",
   "metadata": {},
   "source": [
    "Discuss the results. How is the performance in general? Which $k$ would you chose?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad4de454",
   "metadata": {},
   "source": [
    "## Exercise 6: LDA, QDA & Naïve Bayes\n",
    "\n",
    "Once again, we will use the Iris dataset for classificationa analysis. Your task is to compare the performance of LDA, QDA, and Gaussian Naïve Bayes!\n",
    "\n",
    "1. Load the `iris` dataset from `sklearn.datasets`. We will use only the first two features (sepal length and width)\n",
    "2. `TODO:` Split the data into training and test sets ([use stratification!](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html))\n",
    "3. `TODO:` Fit LDA, QDA, and Naïve Bayes classifiers to the training data and orint the classification report for all models on the test data\n",
    "4. Plot the decision boundaries for both models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8f54c85",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "# 1. Load data\n",
    "iris = load_iris()\n",
    "X = iris.data[:, :2]\n",
    "y = iris.target\n",
    "target_names = iris.target_names\n",
    "\n",
    "# 2. Split into train/test\n",
    "X_train, X_test, y_train, y_test = train_test_split(...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "868f49aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "\n",
    "# 3. TODO: Fit a LDA model and print the classification report\n",
    "lda = ...\n",
    "\n",
    "print(classification_report(y_test, lda.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47376e79",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "\n",
    "# 3. TODO: Fit a QDA model and print the classification report\n",
    "qda = ...\n",
    "\n",
    "print(classification_report(y_test, qda.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "963c6c11",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "# 3. TODO: Fit a Gaussian Naive Bayes model and print the classification report\n",
    "gnb = ...\n",
    "\n",
    "print(classification_report(y_test, gnb.predict(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51fc3321",
   "metadata": {},
   "source": [
    "Once you have trained all three models, you can simply run the following code to plot the decision boundaries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43c7d9cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Plot the decision boundaries for all 3 classifiers\n",
    "\n",
    "# Plotting function\n",
    "def plot_decision_boundary(model, X, y, title, ax):\n",
    "    h = .02\n",
    "    x_min, x_max = X[:, 0].min() - .5, X[:, 0].max() + .5\n",
    "    y_min, y_max = X[:, 1].min() - .5, X[:, 1].max() + .5\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                         np.arange(y_min, y_max, h))\n",
    "    Z = model.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "\n",
    "    cmap_light = ListedColormap(['#FFAAAA', '#AAFFAA', '#AAAAFF'])\n",
    "    cmap_bold = ListedColormap(['#FF0000', '#00FF00', '#0000FF'])\n",
    "\n",
    "    ax.contourf(xx, yy, Z, cmap=cmap_light, alpha=0.2)\n",
    "    scatter = ax.scatter(X[:, 0], X[:, 1], c=y, cmap=cmap_bold, s=30)\n",
    "    ax.set_xlim(xx.min(), xx.max())\n",
    "    ax.set_ylim(yy.min(), yy.max())\n",
    "    ax.set_title(title)\n",
    "    ax.set_xlabel('Sepal length')\n",
    "    ax.set_ylabel('Sepal width')\n",
    "\n",
    "# Create plots for all 3 classifiers\n",
    "fig, axes = plt.subplots(1, 3, figsize=(12, 4))\n",
    "plot_decision_boundary(lda, X_train, y_train, \"LDA Decision Boundary\", axes[0])\n",
    "plot_decision_boundary(qda, X_train, y_train, \"QDA Decision Boundary\", axes[1])\n",
    "plot_decision_boundary(gnb, X_train, y_train, \"Naïve Bayes Decision Boundary\", axes[2])\n",
    "plt.tight_layout()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "psy111",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
